Qcom.cor <- Qcom
Qcom.cor[1,2] <- 2
Qcom.cor[1,3] <- 2
Qindv <- mk_Qindv(Qcom.cor)
Pindv <- mk_Pindv(Qcom.cor, p1=c(0,0,1,0), p2=c(0,1,0,0), p0=c(1,0,0,0))
root.p <- c(1, 0,0,0)
Qcom.cor
out3 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=T, optim.method='subplex', subplex.rounds=1, subplex.n.cores=1L, gensa.its=100)
out3
out3 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=T, optim.method='subplex_annl', subplex.rounds=1, subplex.n.cores=1L, gensa.its=100)
out3
out3 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=T, optim.method='subplex_annl', subplex.rounds=1, subplex.n.cores=1L, gensa.its=300)
Qcom.cor
out3
out1
out2
Qcom <- mk_Qcom(rate1=c(1,1), rate2=c(2,2), names=names(token.maps))
Qindv <- mk_Qindv(Qcom)
Pindv <- mk_Pindv(Qcom, p1=c(0,0,1,0), p2=c(0,1,0,0), p0=c(1,0,0,0))
root.p <- c(1, 0,0,0)
out2 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=F, optim.method='subplex', subplex.rounds=1, subplex.n.cores=1L, gensa.its=100)
out2
Qcom <- mk_Qcom(rate1=c(1,2), rate2=c(3,4), names=names(token.maps))
Qcom <- mk_Qcom(rate1=c(1,2), rate2=c(3,4), names=names(token.maps))
Qcom <- mk_Qcom(rate1=c(1,2), rate2=c(3,4), names=names(token.maps))
Qcom <- mk_Qcom(rate1=c(1,2), rate2=c(3,4), names=names(token.maps))
Qindv <- mk_Qindv(Qcom)
Pindv <- mk_Pindv(Qcom, p1=c(0,0,1,0), p2=c(0,1,0,0), p0=c(1,0,0,0))
root.p <- c(1, 0,0,0)
out2 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=F, optim.method='subplex', subplex.rounds=1, subplex.n.cores=1L, gensa.its=100)
out2
out2 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=T, optim.method='subplex', subplex.rounds=1, subplex.n.cores=1L, gensa.its=100)
out2
out2 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=F, optim.method='subplex_annl', subplex.rounds=1, subplex.n.cores=1L, gensa.its=100)
out2
out2 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=F, optim.method='subplex', subplex.rounds=10, subplex.n.cores=10L, gensa.its=100)
out2
out2$solution.subplex.list
out2 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=F, optim.method='subplex', subplex.rounds=1, subplex.n.cores=1L, gensa.its=100)
out2
out1
Qcom.cor <- Qcom
Qcom.cor
Qcom.cor <- mk_Qcom(rate1=c(1,1), rate2=c(1,1), names=names(token.maps))
Qcom.cor[1,2] <- 2
Qcom.cor[1,3] <- 2
Qindv <- mk_Qindv(Qcom.cor)
Pindv <- mk_Pindv(Qcom.cor, p1=c(0,0,1,0), p2=c(0,1,0,0), p0=c(1,0,0,0))
root.p <- c(1, 0,0,0)
out3 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=T, optim.method='subplex_annl', subplex.rounds=1, subplex.n.cores=1L, gensa.its=300)
out3 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=F, optim.method='subplex', subplex.rounds=1, subplex.n.cores=1L, gensa.its=300)
out3
#out2$solution.subplex.list
#-----
Qcom.cor <- mk_Qcom(rate1=c(1,1), rate2=c(1,1), names=names(token.maps))
Qcom.cor[Qcom.cor==1] <- c(1:8)
Qindv <- mk_Qindv(Qcom.cor)
Pindv <- mk_Pindv(Qcom.cor, p1=c(0,0,1,0), p2=c(0,1,0,0), p0=c(1,0,0,0))
root.p <- c(1, 0,0,0)
out3 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=F, optim.method='subplex', subplex.rounds=1, subplex.n.cores=1L, gensa.its=100)
out3
out3
Qcom.cor <- mk_Qcom(rate1=c(1,1), rate2=c(1,1), names=names(token.maps))
Qcom.cor[1,2] <- 2
Qcom.cor[1,3] <- 2
Qindv <- mk_Qindv(Qcom.cor)
Pindv <- mk_Pindv(Qcom.cor, p1=c(0,0,1,0), p2=c(0,1,0,0), p0=c(1,0,0,0))
root.p <- c(1, 0,0,0)
out4 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=F, optim.method='subplex', subplex.rounds=1, subplex.n.cores=1L, gensa.its=300)
out4
out3
out4
Qcom.cor <- mk_Qcom(rate1=c(1,1), rate2=c(1,1), names=names(token.maps))
Qcom.cor[1,2] <- 2
Qcom.cor[1,3] <- 2
Qcom.cor[3,1] <- 3
Qcom.cor[2,1] <- 3
Qcom.cor
Qindv <- mk_Qindv(Qcom.cor)
Pindv <- mk_Pindv(Qcom.cor, p1=c(0,0,1,0), p2=c(0,1,0,0), p0=c(1,0,0,0))
root.p <- c(1, 0,0,0)
out4 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=F, optim.method='subplex', subplex.rounds=1, subplex.n.cores=1L, gensa.its=300)
out4
Qcom.cor
out2
out1
out4
out3
Qcom.cor <- mk_Qcom(rate1=c(1,1), rate2=c(1,1), names=names(token.maps))
Qcom.cor[1,2] <- 0
Qcom.cor[1,3] <- 2
Qcom.cor[3,1] <- 0
Qcom.cor[2,1] <- 3
Qindv <- mk_Qindv(Qcom.cor)
Pindv <- mk_Pindv(Qcom.cor, p1=c(0,0,1,0), p2=c(0,1,0,0), p0=c(1,0,0,0))
root.p <- c(1, 0,0,0)
out4 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=F, optim.method='subplex', subplex.rounds=1, subplex.n.cores=1L, gensa.its=300)
out4
Qcom.cor <- mk_Qcom(rate1=c(1,1), rate2=c(1,1), names=names(token.maps))
Qcom.cor[1,2] <- 0
Qcom.cor[1,3] <- 2
Qcom.cor[3,1] <- 0
Qcom.cor[2,1] <- 2
Qindv <- mk_Qindv(Qcom.cor)
Pindv <- mk_Pindv(Qcom.cor, p1=c(0,0,1,0), p2=c(0,1,0,0), p0=c(1,0,0,0))
root.p <- c(1, 0,0,0)
out4 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=F, optim.method='subplex', subplex.rounds=1, subplex.n.cores=1L, gensa.its=300)
out4
Qcom.cor <- mk_Qcom(rate1=c(1,1), rate2=c(1,1), names=names(token.maps))
Qcom.cor[1,2] <- 2
Qcom.cor[1,3] <- 2
Qcom.cor[3,1] <- 3
Qcom.cor[2,1] <- 3
Qindv <- mk_Qindv(Qcom.cor)
Pindv <- mk_Pindv(Qcom.cor, p1=c(0,0,1,0), p2=c(0,1,0,0), p0=c(1,0,0,0))
root.p <- c(1, 0,0,0)
out4 <- runSinba(phy=scen$tree, data=scen$data, edge.pattern=NULL,
root.p = root.p, Qindv=Qindv, Pindv=Pindv,
p = NULL, ip = NULL, lb = 0, ub = 100,
cub.method='pcubature', verbose = TRUE, diagn=F, optim.method='subplex', subplex.rounds=1, subplex.n.cores=1L, gensa.its=300)
out4
install.packages("partition")
install.packages("partitions")
library(PhenoTraits)
load("~/Documents/My_papers/Semantic_discriptions/Protege/OWLAPI/Session_Jan10.RData")
load("~/Documents/My_papers/Semantic_discriptions/Protege/OWLAPI/HowTo.R")
install.packages("reticulate")
library(reticulate)
use_python("/usr/local/bin/python3", required = TRUE)
owl<- import("owlready2")
repl_python()
#-------
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/OWLAPI")
repl_python()
remotes::install_github('coolbutuseless/minilexer')
install.packages("ontologyIndex")
devtools::install_github("lorenzwalthert/strcode")
r paste(Sys.Date())
library("minilexer")
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/OWLAPI")
source('parser_functions.R')
source_python('py_func.R')
library("ontologyIndex")
#devtools::install_github("gadenbuie/regexplain")
#library("regexplain")
library("igraph")
library("stringr")
library("dplyr")
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/PhenoScript_R/PhenoScript/Dichotomius_paper")
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/PhenoScript_R/PhenoScript/Dichotomius_paper/Description")
descr <- readLines('Dichotomius_spp_nov')
descr <- readLines('D_annin.txt')
descr
# remove comments
descr <- str_remove(descr, '#.+')
# merge
descr <- paste0(descr, collapse = ' ')
# remove 2 or more whitespaces
descr <-str_squish(descr)
descr <-gsub('\n', '', descr)
descr <-strsplit(descr, '\\{')
descr <-descr[[1]][-1]
descr <-strsplit(descr, '\\}')
#
sp_names <- descr[[1]][1]
descr <-descr[[1]][2]
# remove unnecessary white speces (Ws, Ws)
#str_view_all(descr, '\\(\\s')
descr <-str_replace_all(descr, '\\s\\)', '\\)')
descr <-str_replace_all(descr, '\\(\\s', '\\(')
# change not() to Not()
descr <-str_replace_all(descr, 'not\\(', 'Not\\(')
sp_descr <- lapply(descr, function(x) strsplit(x, ';')[[1]] )
# remove white at the beginning
sp_descr <-lapply(sp_descr, function(x) trimws(x))
names(sp_descr) <- sp_names[[1]]
sp_descr
# ADD HAS PART
sp_descr[[1]] <- sapply(sp_descr[[1]], function(x) add_HasPart_ophu(x), USE.NAMES = F )
sp_descr
man.syn <- sapply(sp_descr[[1]], function(x) create_ophus(x), USE.NAMES = F)
create_ophus <- function(ophu){
json.graph <- PhenSci2json(ophu, reserved.words.namespace='owl:', prop.namespace='obo:', id.prefix = '_test_',
brackets.new = c('{', '}') )
triple.ophu <- rdflib::rdf_parse(json.graph, "jsonld")
g <- triple2graph(triple.ophu)
Manch <- graph2Manchester(g, qulifier='.some', prefix.remove='localhost:///', numeric.prefix='xsd:float')
return(Manch)
}
#ophu <- sp_descr[[1]][21]
create_ophus_E <- function(ophu){
json.graph <- PhenSci2json(ophu, reserved.words.namespace='owl:', prop.namespace='obo:', id.prefix = '_test_',
brackets.new = c('{', '}') )
#cat(json.graph)
triple.ophu <- rdflib::rdf_parse(json.graph, "jsonld")
#g <- triple2graph(triple.ophu)
g <- leave_triple_entity(triple.ophu)
#plot(g)
Manch <- graph2Manchester(g, qulifier='.some', prefix.remove='localhost:///', numeric.prefix='xsd:float')
return(Manch)
}
man.syn <- sapply(sp_descr[[1]], function(x) create_ophus(x), USE.NAMES = F)
install.packages("rdflib")
library("rdflib", lib.loc="~/Library/R/3.6/library")
man.syn <- sapply(sp_descr[[1]], function(x) create_ophus(x), USE.NAMES = F)
library("jsonld", lib.loc="~/Library/R/3.6/library")
library("tibble", lib.loc="~/Library/R/3.6/library")
library("tidyr", lib.loc="~/Library/R/3.6/library")
man.syn <- sapply(sp_descr[[1]], function(x) create_ophus(x), USE.NAMES = F)
library("redland", lib.loc="~/Library/R/3.6/library")
man.syn <- sapply(sp_descr[[1]], function(x) create_ophus(x), USE.NAMES = F)
#--- onto names for translations
# see Snippets.R, object 'all'
setwd('~/Documents/My_papers/Semantic_discriptions/Protege/Ontologies')
all <- read.csv(file='all_ontologies_tibble4Snippets.csv', stringsAsFactors = F)
all <-as_tibble(all)
all
# make lables for onto translations
all <-all %>% mutate(label.transl=label.final)
# add dot to properties and data_props
all <-all %>% mutate(label.transl=replace(label.transl, type=='prop', paste0(filter(all, type=='prop')$label.transl, '.') ))
all <-all %>% mutate(label.transl=replace(label.transl, type=='data_prop', paste0(filter(all, type=='data_prop')$label.transl, '.') ))
# add dot to IDs of properties and data_props
all <-all %>% mutate(ID.transl=ID)
all <-all %>% mutate(ID.transl=replace(ID.transl, type=='prop', paste0(filter(all, type=='prop')$ID.transl, '.') ))
all <-all %>% mutate(ID.transl=replace(ID.transl, type=='data_prop', paste0(filter(all, type=='data_prop')$ID.transl, '.') ))
filter(all, type=='prop')
filter(all, type=='prop') %>% select(label.transl, ID.transl)
all$label.transl
all$ID.transl
all$label.transl[all$label.transl=='SCARAB.has_measurement.']
# Remove SCARAB in labels
all$label.transl <- gsub('SCARAB.', '', all$label.transl)
all$ID.transl <- gsub('SCARAB.', 'scarab.', all$ID.transl)
all$label.transl[all$label.transl=='has_measurement.']
# make ttranslation obj
ont.transl <- all$label.transl
names(ont.transl) <-all$ID.transl
#----
ophu <- 'fore_wing has_part.(wing_vein bearer_of.(yellow))'
json.graph <- PhenSci2json(ophu, reserved.words.namespace='owl:', prop.namespace='obo:', id.prefix = '_test_',
brackets.new = c('{', '}') )
cat(json.graph)
triple.ophu <- rdflib::rdf_parse(json.graph, "jsonld")
trait_type(triple.ophu)
library("readr", lib.loc="~/Library/R/3.6/library")
trait_type(triple.ophu)
remove.packages("readr", lib="~/Library/R/3.6/library")
install.packages("readr")
#devtools::install_github('coolbutuseless/minilexer')
library(reticulate)
library("minilexer")
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/OWLAPI")
source('parser_functions.R')
source_python('py_func.R')
library("ontologyIndex")
#devtools::install_github("gadenbuie/regexplain")
#library("regexplain")
library("igraph")
library("stringr")
library("dplyr")
library("rdflib")
library(rdflib)
library(dplyr)
library(tidyr)
library(tibble)
library(jsonld)
library("redland")
#--- onto names for translations
# see Snippets.R, object 'all'
setwd('~/Documents/My_papers/Semantic_discriptions/Protege/Ontologies')
all
ophu
json.graph <- PhenSci2json(ophu, reserved.words.namespace='owl:', prop.namespace='obo:', id.prefix = '_test_',
brackets.new = c('{', '}') )
cat(json.graph)
triple.ophu <- rdflib::rdf_parse(json.graph, "jsonld")
trait_type(triple.ophu)
g <- triple2graph(triple.ophu)
g
plot(g)
Manch <- graph2Manchester(g, qulifier='.some', prefix.remove='localhost:///', numeric.prefix='xsd:float',
escaped.words = c('not', 'min', 'max'))
translate2URIs_oneToken(Manch, ont.transl, Manchester.pattern)
Manch
descr
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/PhenoScript_R/PhenoScript/Dichotomius_paper/Description")
descr <- readLines('D_annin.txt')
# remove comments
descr <- str_remove(descr, '#.+')
# merge
descr <- paste0(descr, collapse = ' ')
# remove 2 or more whitespaces
descr <-str_squish(descr)
#str_view_all(descr, '\\s')
descr <-gsub('\n', '', descr)
descr <-strsplit(descr, '\\{')
descr <-descr[[1]][-1]
descr <-strsplit(descr, '\\}')
#
sp_names <- descr[[1]][1]
descr <-descr[[1]][2]
# remove unnecessary white speces (Ws, Ws)
descr <-str_replace_all(descr, '\\s\\)', '\\)')
descr <-str_replace_all(descr, '\\(\\s', '\\(')
# change not() to Not()
descr <-str_replace_all(descr, 'not\\(', 'Not\\(')
sp_descr <- lapply(descr, function(x) strsplit(x, ';')[[1]] )
# remove white at the beginning
sp_descr <-lapply(sp_descr, function(x) trimws(x))
names(sp_descr) <- sp_names[[1]]
# ADD HAS PART
sp_descr[[1]] <- sapply(sp_descr[[1]], function(x) add_HasPart_ophu(x), USE.NAMES = F )
sp_descr
create_ophus <- function(ophu){
json.graph <- PhenSci2json(ophu, reserved.words.namespace='owl:', prop.namespace='obo:', id.prefix = '_test_',
brackets.new = c('{', '}') )
triple.ophu <- rdflib::rdf_parse(json.graph, "jsonld")
g <- triple2graph(triple.ophu)
Manch <- graph2Manchester(g, qulifier='.some', prefix.remove='localhost:///', numeric.prefix='xsd:float')
return(Manch)
}
#ophu <- sp_descr[[1]][21]
create_ophus_E <- function(ophu){
json.graph <- PhenSci2json(ophu, reserved.words.namespace='owl:', prop.namespace='obo:', id.prefix = '_test_',
brackets.new = c('{', '}') )
#cat(json.graph)
triple.ophu <- rdflib::rdf_parse(json.graph, "jsonld")
#g <- triple2graph(triple.ophu)
g <- leave_triple_entity(triple.ophu)
#plot(g)
Manch <- graph2Manchester(g, qulifier='.some', prefix.remove='localhost:///', numeric.prefix='xsd:float')
return(Manch)
}
man.syn <- sapply(sp_descr[[1]], function(x) create_ophus(x), USE.NAMES = F)
man.syn
owl.syn <- sapply(man.syn, function(x) translate2URIs_oneToken(x, ont.transl, Manchester.pattern), USE.NAMES = F)
owl.syn
Manch
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/PhenoScript_R/PhenoScript/Dichotomius_paper/Ontologies")
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/PhenoScript_R/PhenoScript/Dichotomius_paper/Ontologies")
repl_python()
owl.syn
#for (j in 1:3){
for (i in 1:length(owl.syn)){
#  for (i in 1:5){
# CL1 <- py_make_class(class_id=paste0('ophu_sp_',j,'_', i), subclass_of='scarab.OPHU_EQ', restriction='is_a', def=owl.syn[i])
# py_run_string(CL1, local = FALSE, convert = F)
CL1 <- py_make_class(class_id=paste0('ophu_sp_',j,'_', i), subclass_of='scarab.OPHU_EQ', restriction='equivalent_to', def=owl.syn[i])
py_run_string(CL1, local = FALSE, convert = F)
#  }
}
j <- 1
#for (j in 1:3){
for (i in 1:length(owl.syn)){
#  for (i in 1:5){
# CL1 <- py_make_class(class_id=paste0('ophu_sp_',j,'_', i), subclass_of='scarab.OPHU_EQ', restriction='is_a', def=owl.syn[i])
# py_run_string(CL1, local = FALSE, convert = F)
CL1 <- py_make_class(class_id=paste0('ophu_sp_',j,'_', i), subclass_of='scarab.OPHU_EQ', restriction='equivalent_to', def=owl.syn[i])
py_run_string(CL1, local = FALSE, convert = F)
#  }
}
py_run_string('scarab.save(file = "SCARAB_merged.owl", format = "rdfxml")', local = FALSE, convert = F)
man.syn
man.syn.E <- sapply(sp_descr[[1]], function(x) create_ophus_E(x), USE.NAMES = F)
man.syn.E
owl.syn.E <- sapply(man.syn.E, function(x) translate2URIs_oneToken(x, ont.transl, Manchester.pattern), USE.NAMES = F)
owl.syn.E
for (i in 1:length(owl.syn.E)){
CL1 <- py_make_class(class_id=paste0('ophu_E_',i), subclass_of='OPHU_E', restriction='is_a', def=owl.syn.E[i])
py_run_string(CL1, local = FALSE, convert = F)
}
py_run_string(
'with scarab:
class OPHU_E(Thing):
pass', convert = F)
py_run_string(
'with scarab:
class OPHU_E(Thing):
pass', convert = F)
repl_python()
py_run_string('scarab.save(file = "SCARAB_merged.owl", format = "rdfxml")', local = FALSE, convert = F)
triple.ophu
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/PhenoScript_R/PhenoScript/Dichotomius_paper/R_scripts")
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/PhenoScript_R/PhenoScript/Dichotomius_paper/R_scripts")
#devtools::install_github('coolbutuseless/minilexer')
library(reticulate)
library("minilexer")
library("ontologyIndex")
#devtools::install_github("gadenbuie/regexplain")
#library("regexplain")
library("igraph")
library("stringr")
library("dplyr")
library("rdflib")
library(tidyr)
library(tibble)
library(jsonld)
library("redland")
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/PhenoScript_R/PhenoScript/Dichotomius_paper/R_scripts/Functions")
source('parser_functions.R')
source_python('py_func.R')
# Setwd Descritions
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/PhenoScript_R/PhenoScript/Dichotomius_paper/Description")
descr <- readLines('D_annin.txt')
# remove comments
descr <- str_remove(descr, '#.+')
# merge
descr <- paste0(descr, collapse = ' ')
# remove 2 or more whitespaces
descr <-str_squish(descr)
#str_view_all(descr, '\\s')
descr <-gsub('\n', '', descr)
descr <-strsplit(descr, '\\{')
descr <-descr[[1]][-1]
descr <-strsplit(descr, '\\}')
#
sp_names <- descr[[1]][1]
descr <-descr[[1]][2]
# remove unnecessary white speces (Ws, Ws)
descr <-str_replace_all(descr, '\\s\\)', '\\)')
descr <-str_replace_all(descr, '\\(\\s', '\\(')
# change not() to Not()
descr <-str_replace_all(descr, 'not\\(', 'Not\\(')
sp_descr <- lapply(descr, function(x) strsplit(x, ';')[[1]] )
# remove white at the beginning
sp_descr <-lapply(sp_descr, function(x) trimws(x))
names(sp_descr) <- sp_names[[1]]
# ADD HAS PART
sp_descr[[1]] <- sapply(sp_descr[[1]], function(x) add_HasPart_ophu(x), USE.NAMES = F )
all
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/PhenoScript_R/PhenoScript/Dichotomius_paper/R_scripts/data")
#--- onto names for translations
# see Snippets.R, object 'all'
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/PhenoScript_R/PhenoScript/Dichotomius_paper/R_scripts/data")
all <- read.csv(file='all_ontologies_tibble4Snippets.csv', stringsAsFactors = F)
all <-as_tibble(all)
all
# make lables for onto translations
all <-all %>% mutate(label.transl=label.final)
# add dot to properties and data_props
all <-all %>% mutate(label.transl=replace(label.transl, type=='prop', paste0(filter(all, type=='prop')$label.transl, '.') ))
all <-all %>% mutate(label.transl=replace(label.transl, type=='data_prop', paste0(filter(all, type=='data_prop')$label.transl, '.') ))
# add dot to IDs of properties and data_props
all <-all %>% mutate(ID.transl=ID)
all <-all %>% mutate(ID.transl=replace(ID.transl, type=='prop', paste0(filter(all, type=='prop')$ID.transl, '.') ))
all <-all %>% mutate(ID.transl=replace(ID.transl, type=='data_prop', paste0(filter(all, type=='data_prop')$ID.transl, '.') ))
filter(all, type=='prop')
filter(all, type=='prop') %>% select(label.transl, ID.transl)
all$label.transl
all$ID.transl
all$label.transl[all$label.transl=='SCARAB.has_measurement.']
# Remove SCARAB in labels
all$label.transl <- gsub('SCARAB.', '', all$label.transl)
all$ID.transl <- gsub('SCARAB.', 'scarab.', all$ID.transl)
all$label.transl[all$label.transl=='has_measurement.']
# make ttranslation obj
ont.transl <- all$label.transl
names(ont.transl) <-all$ID.transl
man.syn <- sapply(sp_descr[[1]], function(x) create_ophus(x), USE.NAMES = F)
owl.syn <- sapply(man.syn, function(x) translate2URIs_oneToken(x, ont.transl, Manchester.pattern), USE.NAMES = F)
owl.syn
setwd("~/Documents/My_papers/Semantic_discriptions/Protege/PhenoScript_R/PhenoScript/Dichotomius_paper/Ontologies")
repl_python()
